{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7CC34uuNzNxY",
   "metadata": {
    "id": "7CC34uuNzNxY"
   },
   "source": [
    "# New Narratives for old Buildings\n",
    "\n",
    "word2vec modeling; visualize corpora with t-SNE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae",
   "metadata": {
    "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae"
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os, re, csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Dx2o0IVXXqbu",
   "metadata": {
    "id": "Dx2o0IVXXqbu"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7010dda9-acc5-4d90-b175-90012564d13c",
   "metadata": {
    "id": "7010dda9-acc5-4d90-b175-90012564d13c"
   },
   "source": [
    "## Loading the dataset: heritage homes webistes\n",
    "\n",
    "Use google drive or local copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42429d3-63fe-4b79-b341-160057e5dcbc",
   "metadata": {
    "id": "d42429d3-63fe-4b79-b341-160057e5dcbc"
   },
   "outputs": [],
   "source": [
    "# Google drive: \n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbjhZ8nKZtZC",
   "metadata": {
    "id": "bbjhZ8nKZtZC"
   },
   "outputs": [],
   "source": [
    "path = '/content/gdrive/MyDrive/CDA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0f83265-ce97-4e80-aef4-9376f6d26892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local copy:\n",
    "path = '/datasets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QYiHwjcORrPC",
   "metadata": {
    "id": "QYiHwjcORrPC"
   },
   "outputs": [],
   "source": [
    "# Country code: change here between 'NL', 'UK', 'DE', 'FR' and 'IR' \n",
    "cc = 'DE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yCPPY_4I2pIZ",
   "metadata": {
    "id": "yCPPY_4I2pIZ"
   },
   "outputs": [],
   "source": [
    "raw_data_file = path+cc+'_dataset_website-content-crawler.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8697b51f-50a5-4091-9cc1-0aed1308b27d",
   "metadata": {
    "id": "8697b51f-50a5-4091-9cc1-0aed1308b27d"
   },
   "outputs": [],
   "source": [
    "# Import json data from Aipfy scraping\n",
    "df=pd.read_json(raw_data_file)\n",
    "# select only two columns for analysis: url and text\n",
    "df=df[['url','text']]\n",
    "# Print the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rr6hiPQ-4z5O",
   "metadata": {
    "id": "Rr6hiPQ-4z5O"
   },
   "source": [
    "Join all pages from a domain to an entry in the analysis. To do this, add a new column which will contain only the main domain name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_Px9Aoim4-pq",
   "metadata": {
    "id": "_Px9Aoim4-pq"
   },
   "outputs": [],
   "source": [
    "# function to extract the main domain from the url in the dataset\n",
    "def extract_main_domain(url):\n",
    "    if not isinstance(str(url), str):\n",
    "        print('NOT VALID',url)\n",
    "        return None\n",
    "    match = re.findall('(?:\\w+\\.)*\\w+\\.\\w*', str(url)) #'www\\.?([^/]+)'\n",
    "    return match[0].lstrip('www.') if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db9deb-8836-47fb-9f74-28a023bcb5d7",
   "metadata": {
    "id": "47db9deb-8836-47fb-9f74-28a023bcb5d7"
   },
   "outputs": [],
   "source": [
    "# Load the list of domains from a csv file:\n",
    "cc_column = cc+' domains'\n",
    "#print(cc_column)\n",
    "\n",
    "urls = pd.read_csv(path+cc+'_urls.csv')[cc_column].values.tolist()\n",
    "\n",
    "# Extract main domains from nl_urls\n",
    "domains = {extract_main_domain(url) for url in urls if extract_main_domain(url) is not None}\n",
    "\n",
    "# Check if main domains in list_of_links match any domain in nl_domains\n",
    "matching_links = [link for link in df.url if extract_main_domain(link) in domains]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9d2331-fe01-4e94-b984-00ac834a771c",
   "metadata": {
    "id": "cc9d2331-fe01-4e94-b984-00ac834a771c"
   },
   "outputs": [],
   "source": [
    "# Add a new column 'domain' and fill it by applying the extract_main_domain function to the 'url' column\n",
    "df['domain'] = df['url'].apply(extract_main_domain)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08144921-d7be-45ed-8795-1c085fb2640b",
   "metadata": {
    "id": "08144921-d7be-45ed-8795-1c085fb2640b"
   },
   "source": [
    "## Understand meaningful words collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inCepASp67ru",
   "metadata": {
    "id": "inCepASp67ru"
   },
   "source": [
    "#### Preparing the text (stopwords, lemmatization, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rKDwducX7kqD",
   "metadata": {
    "id": "rKDwducX7kqD"
   },
   "outputs": [],
   "source": [
    "# make all stopword files stored in github available in this notebook:\n",
    "!wget 'https://raw.githubusercontent.com/jazoza/cultural-data-analysis/refs/heads/main/stopwords_archive/NL.txt'\n",
    "!wget 'https://raw.githubusercontent.com/jazoza/cultural-data-analysis/refs/heads/main/stopwords_archive/UK.txt'\n",
    "!wget 'https://raw.githubusercontent.com/jazoza/cultural-data-analysis/refs/heads/main/stopwords_archive/DE.txt'\n",
    "!wget 'https://raw.githubusercontent.com/jazoza/cultural-data-analysis/refs/heads/main/stopwords_archive/FR.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_bCxFJzY69_b",
   "metadata": {
    "id": "_bCxFJzY69_b"
   },
   "outputs": [],
   "source": [
    "# load a list of 'stopwords' in the language you are analyzing\n",
    "def get_stopwords_list(stop_file_path):\n",
    "    \"\"\"load stop words \"\"\"\n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return list(frozenset(stop_set))\n",
    "stopwords_path = cc+\".txt\"\n",
    "stopwords = get_stopwords_list(stopwords_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HgNkZORH75QF",
   "metadata": {
    "id": "HgNkZORH75QF"
   },
   "outputs": [],
   "source": [
    "# extend the stopwords list with any other words you want to exclude from analysis\n",
    "special_stop_words = ['nbsp', ' ', '', '—', '\\’s', 'ii', 'iii', 'iiii']\n",
    "stopwords_ext = stopwords+special_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0SuPUQ_4VrfT",
   "metadata": {
    "id": "0SuPUQ_4VrfT"
   },
   "outputs": [],
   "source": [
    "#function to remove non-ascii characters\n",
    "def _removeNonAscii(s): return \"\".join(i for i in s if ord(i)<128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8lCObUihV8Yj",
   "metadata": {
    "id": "8lCObUihV8Yj"
   },
   "outputs": [],
   "source": [
    "#load spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yeR7duSYWD9p",
   "metadata": {
    "id": "yeR7duSYWD9p"
   },
   "outputs": [],
   "source": [
    "#function to clean and lemmatize comments\n",
    "def clean_documents(text):\n",
    "    #remove punctuations\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", str(text))\n",
    "    #use spacy to lemmatize comments\n",
    "    doc = nlp(nopunct, disable=['parser','ner'])\n",
    "    lemma = [token.lemma_ for token in doc]\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7VsB2yq1WObL",
   "metadata": {
    "id": "7VsB2yq1WObL"
   },
   "outputs": [],
   "source": [
    "#apply function to clean and lemmatize comments\n",
    "lemmatized = df.text.map(clean_documents)\n",
    "#make sure to lowercase everything\n",
    "lemmatized = lemmatized.map(lambda x: [word.lower() for word in x])\n",
    "lemmatized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IS3V0pl3Xksv",
   "metadata": {
    "id": "IS3V0pl3Xksv"
   },
   "outputs": [],
   "source": [
    "unlist_documents = [item for items in lemmatized for item in items]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VGoew_tHZLpQ",
   "metadata": {
    "id": "VGoew_tHZLpQ"
   },
   "outputs": [],
   "source": [
    "# save these outputs for later\n",
    "with open(path+'jar/'+cc+'_lemmatized.pickle', 'wb') as handle_l:\n",
    "    pickle.dump(lemmatized, handle_l, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(path+'jar/'+cc+'_unlist_documents.pickle', 'wb') as handle_u:\n",
    "    pickle.dump(unlist_documents, handle_u, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aBr85g3ajFB",
   "metadata": {
    "id": "5aBr85g3ajFB"
   },
   "outputs": [],
   "source": [
    "# load saved pickles\n",
    "with open(path+'jar/'+cc+'_lemmatized.pickle', 'rb') as handle_l:\n",
    "    lemmatized = pickle.load(handle_l)\n",
    "\n",
    "with open(path+'jar/'+cc+'_unlist_documents.pickle', 'rb') as handle_u:\n",
    "    unlist_documents = pickle.load(handle_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbkku34SfpU1",
   "metadata": {
    "id": "Hbkku34SfpU1"
   },
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3IaXW0GXX4X2",
   "metadata": {
    "id": "3IaXW0GXX4X2"
   },
   "outputs": [],
   "source": [
    "# initiate bigrams and trigrams\n",
    "bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "trigrams = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6L2vI56MX9ps",
   "metadata": {
    "id": "6L2vI56MX9ps"
   },
   "outputs": [],
   "source": [
    "# identify all collocations in the flat list of words from all documents\n",
    "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(unlist_documents)\n",
    "trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(unlist_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dCLatTKfJDe",
   "metadata": {
    "id": "4dCLatTKfJDe"
   },
   "outputs": [],
   "source": [
    "# compute basic bigram fequency\n",
    "bigramFreqTable = pd.DataFrame(list(bigramFinder.ngram_fd.items()), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "# compute basic tri fequency\n",
    "trigramFreqTable = pd.DataFrame(list(trigramFinder.ngram_fd.items()), columns=['trigram','freq']).sort_values(by='freq', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W8JjofCAYNdC",
   "metadata": {
    "id": "W8JjofCAYNdC"
   },
   "source": [
    "Find meaningful bi- and tri-grams by filtering adjectives and nouns based on an nltk functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OryUdGaYYMGs",
   "metadata": {
    "id": "OryUdGaYYMGs"
   },
   "outputs": [],
   "source": [
    "#function to filter for ADJ/NN bigrams\n",
    "def rightTypes(ngram):\n",
    "    for word in ngram:\n",
    "        if word in stopwords_ext:\n",
    "            return False\n",
    "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6HFZOAafcqP",
   "metadata": {
    "id": "a6HFZOAafcqP"
   },
   "outputs": [],
   "source": [
    "#filter bigrams\n",
    "filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]\n",
    "filtered_bi[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x_P8w-6-jbJj",
   "metadata": {
    "id": "x_P8w-6-jbJj"
   },
   "outputs": [],
   "source": [
    "def rightTypesTri(ngram):\n",
    "    for word in ngram:\n",
    "        if word in stopwords_ext:\n",
    "            return False\n",
    "    first_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    third_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in first_type and tags[2][1] in third_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VyQRFtyBjp6M",
   "metadata": {
    "id": "VyQRFtyBjp6M"
   },
   "outputs": [],
   "source": [
    "#filter trigrams\n",
    "filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda x: rightTypes(x))]\n",
    "filtered_tri[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TF6gJ1PpgrG8",
   "metadata": {
    "id": "TF6gJ1PpgrG8"
   },
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Use [Latent-dirichlet allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) to extract topics from the text based on a statistical model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LX_B4pBbmJ7g",
   "metadata": {
    "id": "LX_B4pBbmJ7g"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "\n",
    "NUM_TOPICS = 10\n",
    "\n",
    "# data to work with: list of tweets\n",
    "documents = df['text'].tolist()\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=5, max_df=0.9, stop_words=stopwords_ext, lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\n",
    "documents_vectorized = vectorizer.fit_transform(documents)\n",
    "# Build a Latent Dirichlet Allocation Model\n",
    "lda_model = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, learning_method='online')\n",
    "lda_Z = lda_model.fit_transform(documents_vectorized)\n",
    "print(lda_Z.shape)  # (NO_DOCUMENTS, NO_TOPICS)\n",
    "(393104, 10)\n",
    "def print_topics(model, vectorizer, top_n=10):\n",
    "    for idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (idx))\n",
    "        print([(vectorizer.get_feature_names_out()[i], topic[i])\n",
    "                        for i in topic.argsort()[:-top_n - 1:-1]])\n",
    "\n",
    "print(\"LDA Model:\")\n",
    "print_topics(lda_model, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y0V0l_03ouQW",
   "metadata": {
    "id": "y0V0l_03ouQW"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(f\"Topic {topic_idx}:\")\n",
    "        print(\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "\n",
    "documents = df['text'].tolist()\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=1000, stop_words=stopwords_ext) # Modify stopwords if needed\n",
    "dtm = vectorizer.fit_transform(documents)\n",
    "\n",
    "num_topics = 10\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)\n",
    "lda.fit(dtm)\n",
    "\n",
    "\n",
    "no_top_words = 10\n",
    "no_top_documents = 10\n",
    "\n",
    "display_topics(lda.components_, lda.transform(dtm), vectorizer.get_feature_names_out(), documents, no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8WPGBwAnqDEU",
   "metadata": {
    "id": "8WPGBwAnqDEU"
   },
   "source": [
    "#### Visualize topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GDt1_G9mozQl",
   "metadata": {
    "id": "GDt1_G9mozQl"
   },
   "outputs": [],
   "source": [
    "from bokeh.io import push_notebook, show, output_notebook\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.models import ColumnDataSource, LabelSet\n",
    "from bokeh.io import output_notebook\n",
    "output_notebook()\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "documents_vectorized = vectorizer.fit_transform(documents)\n",
    "# plotting documents in 2D\n",
    "svd = TruncatedSVD(n_components=20)\n",
    "documents_2d = svd.fit_transform(documents_vectorized)\n",
    "\n",
    "bok_df = pd.DataFrame(columns=['x', 'y', 'document'])\n",
    "bok_df['x'], bok_df['y'], bok_df['document'] = documents_2d[:,0], documents_2d[:,1], range(len(documents))\n",
    "\n",
    "source = ColumnDataSource(ColumnDataSource.from_df(df))\n",
    "labels = LabelSet(x=\"x\", y=\"y\", text=\"document\", y_offset=8,\n",
    "                  text_font_size=\"8pt\", text_color=\"#555555\",\n",
    "                  source=source, text_align='center')\n",
    "\n",
    "plot=figure(min_width=1000, height=600)\n",
    "#plot = figure(plot_width=1000, plot_height=600)\n",
    "plot.scatter(\"x\", \"y\", size=12, source=source, line_color=\"black\", fill_alpha=0.4)\n",
    "plot.add_layout(labels)\n",
    "show(plot, notebook_handle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BI4Eed9ErtXy",
   "metadata": {
    "id": "BI4Eed9ErtXy"
   },
   "outputs": [],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZMi0VD80rcnM",
   "metadata": {
    "id": "ZMi0VD80rcnM"
   },
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda, documents_vectorized, vectorizer, mds='tsne')\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cRJxcGbsWdv8",
   "metadata": {
    "id": "cRJxcGbsWdv8"
   },
   "source": [
    "## Vectorizing the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8nURhyHN8ymX",
   "metadata": {
    "id": "8nURhyHN8ymX"
   },
   "source": [
    "## Word2Vec model, explore and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4YrTkLDz9hjC",
   "metadata": {
    "id": "4YrTkLDz9hjC"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca25ef7-5a19-41e3-a199-bdde7304e575",
   "metadata": {
    "id": "bca25ef7-5a19-41e3-a199-bdde7304e575"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# X is a list of tokenized texts (i.e. list of lists of tokens)\n",
    "X = [word_tokenize(item) for item in df.text.tolist()]\n",
    "#print(X[0:3])\n",
    "model = gensim.models.Word2Vec(X, min_count=6, vector_size=200) # min_count: how many times a word appears in the corpus; size: number of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QS_aLSZc87XP",
   "metadata": {
    "id": "QS_aLSZc87XP"
   },
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=[\"schloss\"], topn=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kPpqk0TYab3j",
   "metadata": {
    "id": "kPpqk0TYab3j"
   },
   "source": [
    "### Visualize keywords with t-SNE\n",
    "\n",
    "Choose keywords that correspond to your analysis and visualize how they and their closest terms are distributed in the discourse.\n",
    "Use t-SNE to visualize the relations.\n",
    "\n",
    "* [t-Distributed Stochastic Neighbor Embedding (t-SNE)](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) is a technique for dimensionality reduction that is particularly well suited for the visualization of high-dimensional datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Xuyj8RYhXRPz",
   "metadata": {
    "id": "Xuyj8RYhXRPz"
   },
   "outputs": [],
   "source": [
    "keys_role = ['Man', 'Frau']\n",
    "\n",
    "embedding_clusters = []\n",
    "word_clusters = []\n",
    "for word in keys_grave:\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    for similar_word, _ in model.wv.most_similar(word, topn=30):\n",
    "        words.append(similar_word)\n",
    "        embeddings.append(model.wv[similar_word])\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wL-7z_InXSX0",
   "metadata": {
    "id": "wL-7z_InXSX0"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "embedding_clusters = np.array(embedding_clusters)\n",
    "n, m, k = embedding_clusters.shape\n",
    "tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5tgZbwmbWdeT",
   "metadata": {
    "id": "5tgZbwmbWdeT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def tsne_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
    "        x = embeddings[:, 0]\n",
    "        y = embeddings[:, 1]\n",
    "        plt.scatter(x, y, c=color, alpha=a, label=label)\n",
    "        for i, word in enumerate(words):\n",
    "            plt.annotate(word, alpha=0.5, xy=(x[i], y[i]), xytext=(5, 2),\n",
    "                         textcoords='offset points', ha='right', va='bottom', size=8)\n",
    "    plt.legend(loc=4)\n",
    "    plt.title(title)\n",
    "    plt.grid(True)\n",
    "    if filename:\n",
    "        plt.savefig(filename, format='png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "tsne_plot_similar_words('Similar words from '+cc+' heritage houses website', keys_role, embeddings_en_2d, word_clusters, 0.7,\n",
    "                        'similar_words.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HSpDL4QoY4_b",
   "metadata": {
    "id": "HSpDL4QoY4_b"
   },
   "outputs": [],
   "source": [
    "keys_status = ['Arbeit', 'Eigentum', 'Zimmer', 'Garten']\n",
    "\n",
    "# calculate embeddings, using topn number of similar words (change the value to include more or less words)\n",
    "embedding_clusters = []\n",
    "word_clusters = []\n",
    "for word in keys_status:\n",
    "    embeddings = []\n",
    "    words = []\n",
    "    for similar_word, _ in model.wv.most_similar(word, topn=20):\n",
    "        words.append(similar_word)\n",
    "        embeddings.append(model.wv[similar_word])\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jLH-4d_mZbBg",
   "metadata": {
    "id": "jLH-4d_mZbBg"
   },
   "outputs": [],
   "source": [
    "embedding_clusters = np.array(embedding_clusters)\n",
    "n, m, k = embedding_clusters.shape\n",
    "tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32)\n",
    "embeddings_en_2d = np.array(tsne_model_en_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
    "\n",
    "# plot a view with new keywords:\n",
    "tsne_plot_similar_words('Similar words from '+cc+' heritage houses website', keys_status, embeddings_en_2d, word_clusters, 0.7,\n",
    "                        'similar_words.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mRSuATcHYm0U",
   "metadata": {
    "id": "mRSuATcHYm0U"
   },
   "source": [
    "Calculate basic frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2nFPgMYkiygf",
   "metadata": {
    "id": "2nFPgMYkiygf"
   },
   "source": [
    "## 4. Analyse specific collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LoJnS9K7h5Kd",
   "metadata": {
    "id": "LoJnS9K7h5Kd"
   },
   "outputs": [],
   "source": [
    "# search for words from this list or use another list\n",
    "search_words = ['royal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f9482-879c-43f9-a9cd-3b1ca5d7056c",
   "metadata": {
    "id": "032f9482-879c-43f9-a9cd-3b1ca5d7056c"
   },
   "outputs": [],
   "source": [
    "# SCI-KIT method, produces lists of co-occurencies for specific terms\n",
    "def vectorize_text(df):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(df['text'])\n",
    "    return X, vectorizer\n",
    "\n",
    "def find_collocations(text, target_words):\n",
    "    words = text.split()\n",
    "    collocations = []\n",
    "    for i in range(len(words) - 1):\n",
    "        if words[i] in target_words:\n",
    "            collocations.append((words[i], words[i + 1]))\n",
    "        if words[i + 1] in target_words:\n",
    "            collocations.append((words[i + 1], words[i]))\n",
    "    return collocations\n",
    "\n",
    "def get_frequent_collocations(df, most_frequent_words):\n",
    "    collocations = []\n",
    "    for text in df['text']:\n",
    "        collocations.extend(find_collocations(text, most_frequent_words))\n",
    "    collocation_counts = Counter(collocations)\n",
    "    frequent_collocations = {}\n",
    "    for word in most_frequent_words:\n",
    "        word_collocations = {collocation: count for collocation, count in collocation_counts.items() if word in collocation}\n",
    "        frequent_collocations[word] = dict(islice(Counter(word_collocations).most_common(20), 20))\n",
    "    return frequent_collocations\n",
    "\n",
    "def analyze_word_collocations(df):\n",
    "    X, vectorizer = vectorize_text(df)\n",
    "    most_frequent_words = search_words\n",
    "    frequent_collocations = get_frequent_collocations(df, most_frequent_words)\n",
    "    return frequent_collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2657cbe3-aa7e-4e4b-8d68-1d6050cf6f49",
   "metadata": {
    "id": "2657cbe3-aa7e-4e4b-8d68-1d6050cf6f49"
   },
   "outputs": [],
   "source": [
    "collocations = analyze_word_collocations(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbef5d2-983b-49fa-8924-95abd24b2855",
   "metadata": {
    "id": "4fbef5d2-983b-49fa-8924-95abd24b2855"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for word, colloc_dict in collocations.items():\n",
    "   for collocation, count in colloc_dict.items():\n",
    "       #collocation_str = ' '.join(collocation)  # Join collocation words into a single string\n",
    "       data.append([word, collocation[1], count])\n",
    "collocations_df = pd.DataFrame(data, columns=['Word', 'Collocation', 'Count'])\n",
    "print(collocations_df.to_markdown(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ornUSbnUl9fN",
   "metadata": {
    "id": "ornUSbnUl9fN"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "vec_king = wv['king']"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

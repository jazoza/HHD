{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7CC34uuNzNxY",
   "metadata": {
    "id": "7CC34uuNzNxY"
   },
   "source": [
    "# Cultural Data Analysis\n",
    "\n",
    "Introduction to working with datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae",
   "metadata": {
    "id": "76a4e150-cc6f-4878-a32b-e78a1d6426ae"
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import os, re, csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim, nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import Counter\n",
    "from itertools import islice\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7010dda9-acc5-4d90-b175-90012564d13c",
   "metadata": {
    "id": "7010dda9-acc5-4d90-b175-90012564d13c"
   },
   "source": [
    "## Loading the dataset: heritage homes webistes\n",
    "\n",
    "The dataset is stored in a shared google drive:\n",
    "https://drive.google.com/drive/folders/11Shm0edDOiWrOe56fzJQRZi-v_BPSW8E?usp=drive_link\n",
    "\n",
    "Add it to your drive.\n",
    "\n",
    "To access it, load your gdrive in 'Files' (see left pane of the notebook in google colab) and navigate to the shared folder. You may need to click on 'refresh' to make it appear on the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42429d3-63fe-4b79-b341-160057e5dcbc",
   "metadata": {
    "id": "d42429d3-63fe-4b79-b341-160057e5dcbc"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbjhZ8nKZtZC",
   "metadata": {
    "id": "bbjhZ8nKZtZC"
   },
   "outputs": [],
   "source": [
    "gdrive_path = '/content/gdrive/MyDrive/CDA/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QYiHwjcORrPC",
   "metadata": {
    "id": "QYiHwjcORrPC"
   },
   "outputs": [],
   "source": [
    "# Country code: change here between 'NL' and 'UK'\n",
    "cc_list = ['NL', 'UK', 'DE', 'FR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_Px9Aoim4-pq",
   "metadata": {
    "id": "_Px9Aoim4-pq"
   },
   "outputs": [],
   "source": [
    "# function to extract the main domain from the url in the dataset\n",
    "def extract_main_domain(url):\n",
    "    if not isinstance(str(url), str):\n",
    "        print('NOT VALID',url)\n",
    "        return None\n",
    "    match = re.findall('(?:\\w+\\.)*\\w+\\.\\w*', str(url)) #'www\\.?([^/]+)'\n",
    "    return match[0].lstrip('www.') if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8697b51f-50a5-4091-9cc1-0aed1308b27d",
   "metadata": {
    "id": "8697b51f-50a5-4091-9cc1-0aed1308b27d"
   },
   "outputs": [],
   "source": [
    "# Import json data from Aipfy scraping into 4 separate dataframes\n",
    "df0=pd.read_json(gdrive_path+cc_list[0]+'_dataset_website-content-crawler.json')\n",
    "# select only two columns for analysis: url and text\n",
    "df0=df0[['url','text']]\n",
    "\n",
    "df1=pd.read_json(gdrive_path+cc_list[1]+'_dataset_website-content-crawler.json')\n",
    "# select only two columns for analysis: url and text\n",
    "df1=df1[['url','text']]\n",
    "\n",
    "df2=pd.read_json(gdrive_path+cc_list[2]+'_dataset_website-content-crawler.json')\n",
    "# select only two columns for analysis: url and text\n",
    "df2=df2[['url','text']]\n",
    "\n",
    "df3=pd.read_json(gdrive_path+cc_list[3]+'_dataset_website-content-crawler.json')\n",
    "# select only two columns for analysis: url and text\n",
    "df3=df3[['url','text']]\n",
    "\n",
    "df0.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rr6hiPQ-4z5O",
   "metadata": {
    "id": "Rr6hiPQ-4z5O"
   },
   "source": [
    "Join all pages from a domain to an entry in the analysis. To do this, add a new column which will contain only the main domain name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47db9deb-8836-47fb-9f74-28a023bcb5d7",
   "metadata": {
    "id": "47db9deb-8836-47fb-9f74-28a023bcb5d7"
   },
   "outputs": [],
   "source": [
    "# Add a new column 'domain' and fill it by applying the extract_main_domain function to the 'url' column\n",
    "\n",
    "# first, create a mapping of dataframes which could be addressed in a loop\n",
    "df_dict = {'0':df0, '1':df1, '2':df2, '3':df3}\n",
    "\n",
    "# then, loop through the df_dict to update each dataframe\n",
    "for k, v in df_dict.items():\n",
    "  cc_column = cc_list[int(k[-1])]+' domains'\n",
    "  cc = cc_list[int(k[-1])]\n",
    "  # print(cc_column, cc)\n",
    "  urls = pd.read_csv(gdrive_path+cc_list[int(k[-1])]+'_urls.csv')[cc_column].values.tolist()\n",
    "  domains = {extract_main_domain(url) for url in urls if extract_main_domain(url) is not None}\n",
    "  matching_links = [link for link in v.url if extract_main_domain(link) in domains]\n",
    "  # update the dataframe\n",
    "  v['domain'] = v['url'].apply(extract_main_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NnGrMALwClPC",
   "metadata": {
    "id": "NnGrMALwClPC"
   },
   "outputs": [],
   "source": [
    "# check one of the dataframes\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inCepASp67ru",
   "metadata": {
    "id": "inCepASp67ru"
   },
   "source": [
    "#### Preparing the text (frequencies, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rKDwducX7kqD",
   "metadata": {
    "id": "rKDwducX7kqD"
   },
   "outputs": [],
   "source": [
    "# make all stopword files stored in github available in this notebook:\n",
    "!wget 'https://raw.githubusercontent.com/jazoza/cultural-data-analysis/refs/heads/main/stopwords_archive/NL.txt'\n",
    "!wget 'https://raw.githubusercontent.com/jazoza/cultural-data-analysis/refs/heads/main/stopwords_archive/UK.txt'\n",
    "!wget 'https://raw.githubusercontent.com/jazoza/cultural-data-analysis/refs/heads/main/stopwords_archive/DE.txt'\n",
    "!wget 'https://raw.githubusercontent.com/jazoza/cultural-data-analysis/refs/heads/main/stopwords_archive/FR.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_bCxFJzY69_b",
   "metadata": {
    "id": "_bCxFJzY69_b"
   },
   "outputs": [],
   "source": [
    "# load a list of 'stopwords' in the language you are analyzing\n",
    "def get_stopwords_list(stop_file_path):\n",
    "    \"\"\"load stop words \"\"\"\n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return list(frozenset(stop_set))\n",
    "\n",
    "sw_dict = {}\n",
    "for i, cc in enumerate(cc_list):\n",
    "  stopwords_path = cc + \".txt\"\n",
    "  sw_dict[str(i)] = get_stopwords_list(stopwords_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39If5_ha2GL",
   "metadata": {
    "id": "b39If5_ha2GL"
   },
   "outputs": [],
   "source": [
    "sw_dict.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MGLqtnvCGH8S",
   "metadata": {
    "id": "MGLqtnvCGH8S"
   },
   "outputs": [],
   "source": [
    "# convert the list of lists into one comprehensive list of stopwords in all languages\n",
    "def flatten_comprehension(dict):\n",
    "  return [item for row in dict for item in row]\n",
    "\n",
    "sw_all = flatten_comprehension(sw_dict.values())\n",
    "# verify if the result is a 1-word string by printing, for example, the 104th item\n",
    "print(sw_all[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HgNkZORH75QF",
   "metadata": {
    "id": "HgNkZORH75QF"
   },
   "outputs": [],
   "source": [
    "# extend the stopwords list with any other words you want to exclude from analysis\n",
    "special_stop_words = ['nbsp', ' ', '', '—', '\\’s', 'ii', 'iii', 'iiii', 'www']\n",
    "sw_all.extend(special_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0SuPUQ_4VrfT",
   "metadata": {
    "id": "0SuPUQ_4VrfT"
   },
   "outputs": [],
   "source": [
    "#function to remove non-ascii characters\n",
    "def _removeNonAscii(s): return \"\".join(i for i in s if ord(i)<128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TTEuzWvOK2Zp",
   "metadata": {
    "id": "TTEuzWvOK2Zp"
   },
   "source": [
    "## Compare corpus structure across corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cSGHLhDxPxIH",
   "metadata": {
    "id": "cSGHLhDxPxIH"
   },
   "outputs": [],
   "source": [
    "# Calculate total number of words, number of words without stopwords (according to the list sw_all) and number of unique words\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "corpus_structure = {}\n",
    "\n",
    "for key, df in df_dict.items():\n",
    "  vectorizer = CountVectorizer(stop_words=None)\n",
    "  X = vectorizer.fit_transform(df['text'])\n",
    "  # total number of words\n",
    "  total_words = X.sum()\n",
    "  # number of words without stopwords\n",
    "  vectorizer_no_stopwords = CountVectorizer(stop_words=sw_all)\n",
    "  X_no_stopwords = vectorizer_no_stopwords.fit_transform(df['text'])\n",
    "  words_without_stopwords = X_no_stopwords.sum()\n",
    "  # number of unique words\n",
    "  unique_words = len(vectorizer.vocabulary_)\n",
    "\n",
    "  corpus_structure[key] = {\n",
    "      'total_words': total_words,\n",
    "      'words_without_stopwords': words_without_stopwords,\n",
    "      'unique_words': unique_words\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9pyjsOZocfaB",
   "metadata": {
    "id": "9pyjsOZocfaB"
   },
   "outputs": [],
   "source": [
    "corpus_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8KT4JyLZQBJ2",
   "metadata": {
    "id": "8KT4JyLZQBJ2"
   },
   "outputs": [],
   "source": [
    "# Print or use the results\n",
    "for key, value in corpus_structure.items():\n",
    "    print(f\"Corpus {key}:\")\n",
    "    print(f\"  Total words: {value['total_words']}\")\n",
    "    print(f\"  Words without stopwords: {value['words_without_stopwords']}\")\n",
    "    print(f\"  Unique words: {value['unique_words']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bwU3GrNeSW6Z",
   "metadata": {
    "id": "bwU3GrNeSW6Z"
   },
   "outputs": [],
   "source": [
    "for k,v in corpus_structure.items():\n",
    "  print(list(v.values())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E9SX5iyHVQnd",
   "metadata": {
    "id": "E9SX5iyHVQnd"
   },
   "outputs": [],
   "source": [
    "# Visualize corpus structure per country\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Extract data for plotting\n",
    "labels = cc_list # Use cc_list for labels\n",
    "total_words = [corpus_structure[str(i)]['total_words'] for i in range(len(cc_list))]\n",
    "words_without_stopwords = [corpus_structure[str(i)]['words_without_stopwords'] for i in range(len(cc_list))]\n",
    "unique_words = [corpus_structure[str(i)]['unique_words'] for i in range(len(cc_list))]\n",
    "\n",
    "x = range(len(labels))\n",
    "width = 0.2\n",
    "\n",
    "plt.bar(x, total_words, width, label='Total Words')\n",
    "plt.bar([i + width for i in x], words_without_stopwords, width, label='Words without Stopwords')\n",
    "plt.bar([i + 2 * width for i in x], unique_words, width, label='Unique Words')\n",
    "\n",
    "\n",
    "plt.xticks([i + width for i in x], labels)\n",
    "plt.xlabel('Country Code')\n",
    "plt.ylabel('Word Count')\n",
    "plt.title('Corpus Structure Comparison')\n",
    "plt.legend()\n",
    "plt.tight_layout() # improve spacing\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1owhsfcBUEKh",
   "metadata": {
    "id": "1owhsfcBUEKh"
   },
   "outputs": [],
   "source": [
    "# Visualize counties in corpus structure\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "categories = ['total_words', 'words_without_stopwords', 'unique_words']\n",
    "bar_width = 0.2\n",
    "\n",
    "for i, (key, value) in enumerate(corpus_structure.items()):\n",
    "    x_pos = np.arange(len(categories)) + i * bar_width\n",
    "    plt.bar(x_pos, list(value.values()), width=bar_width, label=cc_list[int(key)]) # Use cc_list for labels\n",
    "\n",
    "plt.xticks(np.arange(len(categories)) + bar_width, categories)\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Corpus Structure')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08144921-d7be-45ed-8795-1c085fb2640b",
   "metadata": {
    "id": "08144921-d7be-45ed-8795-1c085fb2640b"
   },
   "source": [
    "## Compare collocations across corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aBr85g3ajFB",
   "metadata": {
    "id": "5aBr85g3ajFB"
   },
   "outputs": [],
   "source": [
    "# load saved pickles\n",
    "lemma_dict = {}\n",
    "for i, cc in enumerate(cc_list):\n",
    "  lemma_path = gdrive_path+'jar/'+cc+'_unlist_documents.pickle'\n",
    "  with open(lemma_path, 'rb') as handle_u:\n",
    "    lemma_dict[str(i)] = pickle.load(handle_u)\n",
    "\n",
    "len(lemma_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L-d6TF5iIQNc",
   "metadata": {
    "id": "L-d6TF5iIQNc"
   },
   "outputs": [],
   "source": [
    "# check if word 'kasteel' appears in one of the lemma_dict items '0'='NL', '1'='UK', '2'='DE', '3'='FR']\n",
    "if 'kasteel' in lemma_dict['3']:\n",
    "  print('yes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B7_jU_neI5dz",
   "metadata": {
    "id": "B7_jU_neI5dz"
   },
   "outputs": [],
   "source": [
    "lemma_all = flatten_comprehension(lemma_dict.values())\n",
    "# verify if the result is a 1-word string by printing, for example, the 1044th word\n",
    "# in the comprehensive flat list of all lemmatized words in the four languages\n",
    "print(sw_all[1044])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hbkku34SfpU1",
   "metadata": {
    "id": "Hbkku34SfpU1"
   },
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3IaXW0GXX4X2",
   "metadata": {
    "id": "3IaXW0GXX4X2"
   },
   "outputs": [],
   "source": [
    "# initiate bigrams and trigrams\n",
    "bigrams = nltk.collocations.BigramAssocMeasures()\n",
    "trigrams = nltk.collocations.TrigramAssocMeasures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6L2vI56MX9ps",
   "metadata": {
    "id": "6L2vI56MX9ps"
   },
   "outputs": [],
   "source": [
    "# identify all collocations in the flat list of words from all documents\n",
    "bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(lemma_all)\n",
    "trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(lemma_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dCLatTKfJDe",
   "metadata": {
    "id": "4dCLatTKfJDe"
   },
   "outputs": [],
   "source": [
    "# compute basic bigram fequency\n",
    "bigramFreqTable = pd.DataFrame(list(bigramFinder.ngram_fd.items()), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n",
    "# compute basic tri fequency\n",
    "trigramFreqTable = pd.DataFrame(list(trigramFinder.ngram_fd.items()), columns=['trigram','freq']).sort_values(by='freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OryUdGaYYMGs",
   "metadata": {
    "id": "OryUdGaYYMGs"
   },
   "outputs": [],
   "source": [
    "#function to filter for ADJ/NN bigrams\n",
    "def rightTypes(ngram):\n",
    "    for word in ngram:\n",
    "        _removeNonAscii(word)\n",
    "        if word in sw_all:\n",
    "            return False\n",
    "        if len(word) <= 2:\n",
    "            return False\n",
    "    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#filter bigrams\n",
    "filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x_P8w-6-jbJj",
   "metadata": {
    "id": "x_P8w-6-jbJj"
   },
   "outputs": [],
   "source": [
    "def rightTypesTri(ngram):\n",
    "    for word in ngram:\n",
    "        _removeNonAscii(word)\n",
    "        if word in sw_all:\n",
    "            return False\n",
    "        if len(word) <= 2:\n",
    "            return False\n",
    "    first_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    third_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n",
    "    tags = nltk.pos_tag(ngram)\n",
    "    if tags[0][1] in first_type and tags[2][1] in third_type:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "#filter trigrams\n",
    "filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda x: rightTypes(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W8JjofCAYNdC",
   "metadata": {
    "id": "W8JjofCAYNdC"
   },
   "source": [
    "### Find meaningful bi- and tri-grams for specific search words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ui3wC6xHF1jp",
   "metadata": {
    "id": "ui3wC6xHF1jp"
   },
   "outputs": [],
   "source": [
    "# search for words from this list or use another list\n",
    "search_words = ['kasteel', 'castle', 'château', 'schloss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NZeGAFfmpr0A",
   "metadata": {
    "id": "NZeGAFfmpr0A"
   },
   "outputs": [],
   "source": [
    "# empty dictionary to store the four sections of filtered bigrams\n",
    "search_dict = {}\n",
    "for term in search_words:\n",
    "  # Filter for rows where the 'bigram' column contains 'royal'\n",
    "  search_bigrams = filtered_bi[filtered_bi['bigram'].astype(str).str.contains(term)]\n",
    "  search_dict[term] = search_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mcvkj2Y-vcm8",
   "metadata": {
    "id": "mcvkj2Y-vcm8"
   },
   "outputs": [],
   "source": [
    "search_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Y4l8Myl6tM2q",
   "metadata": {
    "id": "Y4l8Myl6tM2q"
   },
   "outputs": [],
   "source": [
    "# empty list to store the top 20 collocations for each term\n",
    "collocations_list = []\n",
    "\n",
    "# Iterate through each term in the search_dict\n",
    "for term, df in search_dict.items():\n",
    "    # Sort the dataframe by frequency in descending order and take the top 50\n",
    "    top_50 = df.sort_values(by='freq', ascending=False).head(50)\n",
    "    # Append the top 50 bigrams as a list to the collocations_list\n",
    "    collocations_list.append(top_50['bigram'].tolist())\n",
    "\n",
    "max_len = max(len(lst) for lst in collocations_list)\n",
    "for i in range(len(collocations_list)):\n",
    "  if len(collocations_list[i]) < max_len:\n",
    "    collocations_list[i].extend([''] * (max_len - len(collocations_list[i])))\n",
    "\n",
    "new_df = pd.DataFrame(collocations_list, index=search_words).T\n",
    "# Rename the columns to match the search terms\n",
    "new_df.columns = search_words\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RYhtt60cvn3U",
   "metadata": {
    "id": "RYhtt60cvn3U"
   },
   "outputs": [],
   "source": [
    "search_tri_dict = {}\n",
    "for term in search_words:\n",
    "  search_trigrams = filtered_tri[filtered_tri['trigram'].astype(str).str.contains(term)]\n",
    "  search_tri_dict[term] = search_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EXqoD84EzHZE",
   "metadata": {
    "id": "EXqoD84EzHZE"
   },
   "outputs": [],
   "source": [
    "# empty list to store the top 20 collocations for each term\n",
    "collocations_list = []\n",
    "\n",
    "# Iterate through each term in the search_dict\n",
    "for term, df in search_tri_dict.items():\n",
    "    # Sort the dataframe by frequency in descending order and take the top 50\n",
    "    top_50 = df.sort_values(by='freq', ascending=False).head(50)\n",
    "    # Append the top 50 bigrams as a list to the collocations_list\n",
    "    collocations_list.append(top_50['trigram'].tolist())\n",
    "\n",
    "max_len = max(len(lst) for lst in collocations_list)\n",
    "for i in range(len(collocations_list)):\n",
    "  if len(collocations_list[i]) < max_len:\n",
    "    collocations_list[i].extend([''] * (max_len - len(collocations_list[i])))\n",
    "\n",
    "new_df = pd.DataFrame(collocations_list, index=search_words).T\n",
    "# Rename the columns to match the search terms\n",
    "new_df.columns = search_words\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9Kr2mQhptuDH",
   "metadata": {
    "id": "9Kr2mQhptuDH"
   },
   "outputs": [],
   "source": [
    "search_tri_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LoAJ6jjrooPZ",
   "metadata": {
    "id": "LoAJ6jjrooPZ"
   },
   "outputs": [],
   "source": [
    "# suggested search words (in Dutch) for further exploration\n",
    "general_words = ['architectuur', 'collectie', 'geschiedenis', 'tuin', 'onderzoek', 'stijl']\n",
    "family_words = ['kinder', 'spel', 'familie', 'koffie', 'lunch', 'kinderfeestjes', 'huwelijk', 'bruid', 'bruidegom', 'high tea']\n",
    "epoch_words = ['eeuw', '12de', '13de', '14de', '15de', '16de', 'Barroke', 'Renaissance', 'Romantiek', 'Verlichting', 'Rococo', 'Middeleeuwen', 'schatkamer', 'Floris', 'droom','hofdame']"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67d692cf-432e-4594-958a-ea38ba08c712",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7415d360-1ea4-45fb-8d1a-5df381f54b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import WebDriverException\n",
    "from urllib.parse import urlparse, urljoin\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a579c8ec-a0fc-47cb-92e6-b32ccd435b6d",
   "metadata": {},
   "source": [
    "## Make lists of URLs per country"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0d678d-b524-462d-a673-4d5b0bd0c810",
   "metadata": {},
   "source": [
    "### NL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83af802b-c0c7-496a-be35-c44bb197347f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl_files_list = os.listdir(\"../met-naam-2018/castles-netherlands/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "01e6284d-b797-48bd-84d4-e483cb6328d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dchar_domain_extenstions = ['net','org','com','top']\n",
    "nl_urls_div = [link[:-4] for link in nl_files_list if link.endswith('.txt') and link[-7:-4] in dchar_domain_extenstions]\n",
    "nl_urls_div = ['https://www.'+link[:-3] + '.' + link[-3:] for link in nl_urls_div]\n",
    "\n",
    "nl_urls_nl = ['https://www.'+item[:-6]+'.'+item.rstrip('.txt')[-2:] for item in nl_files_list]\n",
    "\n",
    "nl_urls = nl_urls_div + nl_urls_nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "68d6786d-0a69-4d5e-b161-c51820ff28d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 ['https://www.artlandt.op']\n"
     ]
    }
   ],
   "source": [
    "print(len(nl_urls), nl_urls[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "71fcf24f-4a4a-4aec-b788-56a96484720a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.slotzuylen.nl',\n",
       " 'https://www.kasteelgeldrop.nl',\n",
       " 'https://www.fraeylemaborg.nl',\n",
       " 'https://www.slotloevestein.nl',\n",
       " 'https://www.museumijsselstein.nl',\n",
       " 'https://www.twickel.nl',\n",
       " 'https://www.kasteelduivenvoorde.nl',\n",
       " 'https://www.huystendonck.nl',\n",
       " 'https://www.kasteeldussen.nl',\n",
       " 'https://www.huisbergh.nl']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nl_urls[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aefaf4b4-ea06-439b-82bd-a84acef840d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 ['https://www.artlandt.op']\n"
     ]
    }
   ],
   "source": [
    "print(len(nl_urls), nl_urls[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54ac626b-e5b6-4f5e-8d1c-61186ee6893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: save this list as a csv file\n",
    "df_nl = pd.DataFrame(data={\"NL domain\": nl_urls})\n",
    "df_nl.to_csv(\"../url_lists/nl_urls.csv\", sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b345d1b0-8760-44a3-ae0a-b59c0fe53072",
   "metadata": {},
   "source": [
    "### FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "1e558de3-3b82-48c6-9b32-27bd62bb803f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_files_list = os.listdir(\"../met-naam-2018/castles-france/\")\n",
    "len(fr_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "b76c5d41-6099-46b2-8810-c878bdb657b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "domain_extensions = ['com', 'net', 'org', 'bzh']\n",
    "fr_urls_div = [link[:-4] for link in fr_files_list if link.endswith('.txt') and link[-7:-4] in domain_extensions]\n",
    "fr_urls_div = ['http://www.'+link[:-3] + '.' + link[-3:] for link in fr_urls_div]\n",
    "\n",
    "\n",
    "fr_urls_fr = [link[:-4] for link in fr_files_list if link.endswith('.txt') and link[-6:-4]== 'fr']\n",
    "fr_urls_fr = ['http://www.'+link[:-2] + '.' + link[-2:] for link in fr_urls_fr]\n",
    "\n",
    "#catch the unusual .corsica domains\n",
    "fr_urls_corsica = [link[:-4] for link in fr_files_list if link.endswith('.txt') and link[-11:-4]== 'corsica']\n",
    "fr_urls_corsica = ['http://'+link[:-7] + '.' + link[-7:] for link in fr_urls_corsica]\n",
    "\n",
    "fr_urls = fr_urls_div + fr_urls_fr + fr_urls_corsica\n",
    "print(len(fr_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "891edf82-d14d-4fe7-84ba-ae79e0c9453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: save this list as a csv file\n",
    "df_fr = pd.DataFrame(data={\"FR domain\": fr_urls})\n",
    "df_fr.to_csv(\"../url_lists/fr_urls.csv\", sep=',',index=False)\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#    print (df_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349ec20d-7273-4ac3-8ff3-65a5b4ee5195",
   "metadata": {},
   "source": [
    "### DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "79fc9f4c-431e-42e9-8b7d-fc626de6d635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_files_list = os.listdir(\"../met-naam-2018/castles-germany/\")\n",
    "len(de_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9a19f536-022c-4aff-adcd-831ba006c570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    }
   ],
   "source": [
    "dchar_domain_extenstions = ['net','org','com']\n",
    "de_urls_div = [link[:-4] for link in de_files_list if link.endswith('.txt') and link[-7:-4] in dchar_domain_extenstions]\n",
    "de_urls_div = ['http://www.'+link[:-3] + '.' + link[-3:] for link in de_urls_div]\n",
    "\n",
    "cchar_domain_extensions = ['de','nl','eu']\n",
    "de_urls_de = [link[:-4] for link in de_files_list if link.endswith('.txt') and link[-6:-4] in cchar_domain_extensions]\n",
    "de_urls_de = ['http://www.'+link[:-2] + '.' + link[-2:] for link in de_urls_de]\n",
    "\n",
    "#catch the .info domain\n",
    "de_urls_info = [link[:-4] for link in de_files_list if link.endswith('.txt') and link[-8:-4]== 'info']\n",
    "de_urls_info = ['http://'+link[:-4] + '.' + link[-4:] for link in de_urls_info]\n",
    "\n",
    "de_urls = de_urls_div + de_urls_de + de_urls_info\n",
    "print(len(de_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "b816047d-0cf9-420c-8311-f1dfb095c59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: save this list as a csv file\n",
    "df_de = pd.DataFrame(data={\"DE domain\": de_urls})\n",
    "df_de.to_csv(\"../de_urls.csv\", sep=',',index=False)\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#    print (df_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732b2dd2-e50e-446c-9ece-a11cf05ceca6",
   "metadata": {},
   "source": [
    "### UK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "6d382742-9d5e-474b-a4d3-62812672da18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uk_files_list = os.listdir(\"../met-naam-2018/castles-united-kingdom/\")\n",
    "len(uk_files_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "6824e2cb-efee-4cb3-b933-915846b80fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    }
   ],
   "source": [
    "dchar_domain_extenstions = ['net','org','com']\n",
    "uk_urls_div = [link[:-4] for link in uk_files_list if link.endswith('.txt') and link[-7:-4] in dchar_domain_extenstions]\n",
    "uk_urls_div = ['https://www.'+link[:-3] + '.' + link[-3:] for link in uk_urls_div]\n",
    "\n",
    "cchar_domain_extensions_5 = ['govuk','orguk']\n",
    "uk_urls_gov = [link[:-4] for link in uk_files_list if link.endswith('.txt') and link[-9:-4] in cchar_domain_extensions_5]\n",
    "uk_urls_gov = ['https://www.'+link[:-5] + '.' + link[-5:-2] + '.' + link[-2:] for link in uk_urls_gov]\n",
    "\n",
    "cchar_domain_extensions_4 = ['couk','acuk']\n",
    "uk_urls_co = [link[:-4] for link in uk_files_list if link.endswith('.txt') and link[-8:-4] in cchar_domain_extensions_4]\n",
    "uk_urls_co = ['https://www.'+link[:-4] + '.' + link[-4:-2] + '.' + link[-2:] for link in uk_urls_co]\n",
    "\n",
    "uk_urls = uk_urls_div + uk_urls_gov + uk_urls_co + ['https://www.hemyockcastle.uk']\n",
    "print(len(uk_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "4fa2c62d-05b4-4427-ab93-32992951a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: save this list as a csv file\n",
    "df_uk = pd.DataFrame(data={\"UK domain\": uk_urls})\n",
    "df_uk.to_csv(\"../uk_urls.csv\", sep=',',index=False)\n",
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#    print (df_uk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e134678-38eb-4643-b96f-fcc488c7825f",
   "metadata": {},
   "source": [
    "## URL Check up functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d9b2b31-fe92-496d-9f27-7f14d1b1b9dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def check_url(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            print(0)\n",
    "        else:\n",
    "            print(f\"The URL '{url}' is unreachable. Status code: {response.status_code}\")\n",
    "    except requests.ConnectionError:\n",
    "        print(f\"The URL '{url}' is unreachable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afbe9a43-a421-40ca-8c2f-1719b953188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = 'FR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96ce5aa3-2f42-48b8-93f9-4a2376ea7ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The URL 'https://www.la-hunaudaye.com/' is unreachable. Status code: 403\n",
      "0\n",
      "0\n",
      "The URL 'https://www.chateau-eaucourt.com' is unreachable. Status code: 429\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "The URL 'https://www.chateau-pirou.fr/' is unreachable. Status code: 410\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "The URL 'https://www.chateau-picquigny.com' is unreachable. Status code: 429\n",
      "The URL 'https://www.chateau-bazoches.com' is unreachable.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "The URL 'https://www.chateau-malbrouck.com' is unreachable.\n",
      "0\n",
      "0\n",
      "0\n",
      "The URL 'https://www.chateau-de-la-rochepot.com' is unreachable.\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "The URL 'https://www.chateaugavray.fr' is unreachable. Status code: 555\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "The URL 'https://www.chateaudetanlay.fr' is unreachable. Status code: 429\n",
      "The URL 'https://www.folleville-chateau-medieval.fr' is unreachable. Status code: 429\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# optional: change between existing url list (made in this notebook) and a locally stored csv list \n",
    "domain = pd.read_csv('../url_lists/'+cc+'_urls.csv')\n",
    "urls = domain['FR domains'].values.tolist() \n",
    "\n",
    "for i in range(len(urls)):\n",
    "    #print(urls[i])\n",
    "    check_url(urls[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab6fa08-05be-4bcf-922f-6d2029005e3f",
   "metadata": {},
   "source": [
    "## Notes on URLS: \n",
    "\n",
    "#### DUTCH CASTLES (XX/55)\n",
    "- https://www.museumdefundatie.nl/ is not on the 2018 list\n",
    "- https://www.beeldentuinravesteyn.nl changed domain to: https://www.ruine-ravesteyn.nl/\n",
    "\n",
    "#### FRENCH CASTLES (6/91)\n",
    "doesn't respond to checkup function (errors 429, 403 and 555) but the link works - it is correct: \n",
    "- https://www.chateau-eaucourt.com/ \n",
    "- http://www.chateau-picquigny.com\n",
    "- https://www.la-hunaudaye.com/\n",
    "- https://www.chateaudetanlay.fr/\n",
    "- https://www.chateaugavray.fr/\n",
    "\n",
    "seems to be closed, link won't open but WHOIS listing exists:\n",
    "- www.chateau-de-la-rochepot.com (wikipedia: https://en.wikipedia.org/wiki/Ch%C3%A2teau_de_la_Rochepot \"In October 2018, the castle was seized by the French government after investigation into an alleged money laundering scheme by Dmytro Malynovskyi, a Ukrainian. Malynovski was arrested after an investigation into a corruption and money laundering scheme in which he purchased and lived in the castle after faking his own death.\")\n",
    " \n",
    "#### GERMAN CASTLES (10/90)\n",
    "doesn't respond to checkup function (errors 429 and 403) but the link is correct:\n",
    "- https://www.schlossparkmarisfeld.de/\n",
    "- https://www.schloss-marienburg.de/\n",
    "- https://schloss-langenburg.de/\n",
    "- https://schloss-karlsberg.de/ (secure connection missing or required?)\n",
    "- http://www.schloss-braunfels.de'\n",
    "\n",
    "possible mistakes and changes to domain names:\n",
    "- wwwkasteelbentheimnl -> https://burg-bentheim.de/ text is in dutch in German 2018 scraped material\n",
    "- http://evenburg.landkreis-leer.de/ redirects to http://landkreis-leer.de/ the castle does not have its own domain since 2022, review the subdomain  wmk.landkreis-leer.de/ as replacement castle representation\n",
    "- http://www.wilhelmshoehede.txt requires Flash (very outdated site, no accessible content)\n",
    "- https://www.schloesser-hessen.de/de/schloss-erbach is a collection of castles? thus https://www.schloss-erbach.de redirects here\n",
    "- http://burghohnstein.com is for sale, it is replaces by http://burghohnstein.info which is included in the list here\n",
    "\n",
    "#### UK CASTLES (12/76)\n",
    "doesn't respont to checkup function (error 429, 403):\n",
    "- https://www.hrp.org.uk/ - is a collection of palaces - perhaps there are individual websites?\n",
    "- https://www.hants.gov.uk/\n",
    "- https://www.haddonhall.co.uk/\n",
    "- https://www.tamworthcastle.co.uk/\n",
    "- https://www.whittingtoncastle.co.uk only reachable without 'www' (as https://whittingtoncastle.co.uk/)\n",
    "\n",
    "possible mistake:\n",
    "- why is https://dur.ac.uk/ on the list? it is the university website, no (direct) castle content\n",
    "\n",
    "closed:\n",
    "- https://wresslecastle.org/ domain not maintained (people speak of using Facebook page to inquire about opening hours), Selena removed from url list Nov 2024\n",
    "- https://www.leicestercastle.co.uk domain not maintained, information on the castle available on history of Leicestre website, https://www.storyofleicester.info/\n",
    "\n",
    "changed domains:\n",
    "- http://www.rabycastle.com is now https://www.raby.co.uk \n",
    "- http://www.castlerising.com is now http://www.castlerising.co.uk\n",
    "- http://www.fobt.org.uk is now https://friendsofbuckdentowers.co.uk/\n",
    "- https://www.turtontower.co.uk is now https://turtontower.com/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Notes for writing\n",
    "- visit https://castlestudiestrust.org/ and understand what they mean by 'cutting edge castle research'\n",
    "- narrate how domains differ for countries - NL are all .nl except for 2 .com, while FR, DE and UK have different endings; this incoherency is reflected in the different character of top-level domain endings - some FR, DE and UK casltes have the commercial .com and co.uk ending, while others go with .org and .net which are in principle for non-profit organisations; .de and .fr (and 1 .uk) domains reflect the standard country association, while only some UK castles have gov.uk castles, directly related to the government\n",
    "- General questions:\n",
    "1. what is a web presentation of the castle, what cultural meaning does it cary, how is digital communication important to castle preservation and management?\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4545a50-74c8-4c12-8c12-db619b0719c9",
   "metadata": {},
   "source": [
    "# Scrape function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d1edf4f4-cd5e-4672-9083-3cf83758da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_website(url, visited_urls=set()):\n",
    "    try:\n",
    "        if url in visited_urls:\n",
    "            return \"\"\n",
    "        \n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            visited_urls.add(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            content = soup.get_text()\n",
    "            \n",
    "            # Find all internal links and recursively scrape them\n",
    "            internal_links = soup.find_all('a', href=True)\n",
    "            for link in internal_links:\n",
    "                absolute_url = urljoin(url, link['href'])\n",
    "                if urlparse(absolute_url).netloc == urlparse(url).netloc:\n",
    "                    content += scrape_website(absolute_url, visited_urls)\n",
    "            \n",
    "            return content\n",
    "        elif response.status_code == 403:\n",
    "            print(f\"403 Forbidden: {url}\")\n",
    "            return \"\"\n",
    "        else:\n",
    "            print(f\"Error scraping {url}: {response.status_code}\")\n",
    "            return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def write_to_file(content, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "604c90aa-2ed6-4289-850e-5d6823b0615b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test scrape one website\n",
    "write_path=\"../TEST/\"\n",
    "url = nl_urls[44]\n",
    "\n",
    "def main():\n",
    "    domain = urlparse(url).netloc\n",
    "    filename = write_path+f\"{domain}.txt\"\n",
    "    write_to_file(\"STARTING\", filename)\n",
    "    content = scrape_website(url)\n",
    "    #print(content)\n",
    "    if content:\n",
    "        print('there is', content)\n",
    "        write_to_file(content, filename)\n",
    "        print(f\"Scraped {url} and wrote content to {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to scrape {url}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb9a94b-0bb6-4a76-aeaa-6121cc83c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all Dutch websites or a range of urls list\n",
    "write_path=\"../met-naam-2024/castles-netherlands-1/\"\n",
    "\n",
    "def main():\n",
    "    for url in nl_urls[0:6]:\n",
    "        try:\n",
    "            domain = urlparse(url).netloc\n",
    "            filename = write_path+f\"{domain}.txt\"\n",
    "            content = scrape_website(url)\n",
    "            if content:\n",
    "                write_to_file(content, filename)\n",
    "                print(f\"Scraped {url} and wrote content to {filename}\")\n",
    "            else:\n",
    "                print(f\"Failed to scrape {url}\")\n",
    "            # Introduce a delay between requests to avoid overwhelming the server\n",
    "            time.sleep(2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb122c9-4be9-4aaa-a05a-e98e295b19ff",
   "metadata": {},
   "source": [
    "#### Test one url in Wayback machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad51a158-0684-4b21-b359-9eb67842300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url= \"https://web.archive.org/web/20180511012246/https://www.dekemastate.nl/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aca0b4dd-0ae6-4bcc-bc67-c6fdc9631b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to scrape https://web.archive.org/web/20180511012246/https://www.dekemastate.nl/\n"
     ]
    }
   ],
   "source": [
    "content = scrape_website(url)\n",
    "filename = 'wwwdekemastatenl.txt'\n",
    "if content:\n",
    "    write_to_file(content, filename)\n",
    "    print(f\"Scraped {url} and wrote content to {filename}\")\n",
    "else:\n",
    "    print(f\"Failed to scrape {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "2496f904-7f7d-4525-aedf-27e4a1084400",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup1 = BeautifulSoup(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f28855c1-b287-47cd-9116-6ccb0c70baa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_tags = ['div','p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'span', 'br']\n",
    "all_text = ' '.join([element.get_text() for element in soup.find_all(allowed_tags)])\n",
    "with open(url[29:33], \"w\") as file:\n",
    "    file.write(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecb3d4d-d225-4283-a12e-f9bdc747328f",
   "metadata": {},
   "source": [
    "## CommonCrawl test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0efeeb6-837b-4160-b8af-b92a735f8047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "# For parsing URLs:\n",
    "from urllib.parse import quote_plus\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e039932b-e2e5-4e81-baf0-d0bdea9df36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_cc_index(url, index_name):\n",
    "    \"\"\"\n",
    "    Search the Common Crawl Index for a given URL.\n",
    "\n",
    "    This function queries the Common Crawl Index API to find records related to the specified URL. \n",
    "    It uses the index specified by `index_name` to retrieve the data and returns a list of JSON objects, \n",
    "    each representing a record from the index.\n",
    "\n",
    "    Arguments:\n",
    "        url (str): The URL to search for in the Common Crawl Index.\n",
    "        index_name (str): The name of the Common Crawl Index to search (e.g., \"CC-MAIN-2024-10\").\n",
    "\n",
    "    Returns:\n",
    "        list: A list of JSON objects representing records found in the Common Crawl Index. \n",
    "              Returns None if the request fails or no records are found.\n",
    "\n",
    "    Example:\n",
    "        >>> search_cc_index(\"example.com\", \"CC-MAIN-2024-10\")\n",
    "        [{...}, {...}, ...]\n",
    "    \"\"\"\n",
    "    encoded_url = quote_plus(url)\n",
    "    index_url = f'http://index.commoncrawl.org/{index_name}-index?url={encoded_url}&output=json'\n",
    "    response = requests.get(index_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        records = response.text.strip().split('\\n')\n",
    "        return [json.loads(record) for record in records]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def fetch_single_record(warc_record_filename, offset, length):\n",
    "    \"\"\"\n",
    "    Fetch a single WARC record from Common Crawl.\n",
    "\n",
    "    Arguments:\n",
    "        record {dict} -- A dictionary containing the WARC record details.\n",
    "\n",
    "    Returns:\n",
    "        bytes or None -- The raw content of the response if found, otherwise None.\n",
    "    \"\"\"\n",
    "    \n",
    "    s3_url = f'https://data.commoncrawl.org/{warc_record_filename}'\n",
    "\n",
    "    # Define the byte range for the request\n",
    "    byte_range = f'bytes={offset}-{offset + length - 1}'\n",
    "\n",
    "    # Send the HTTP GET request to the S3 URL with the specified byte range\n",
    "    response = requests.get(\n",
    "        s3_url,\n",
    "        headers={'Range': byte_range},\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    if response.status_code == 206:\n",
    "        # Use `stream=True` in the call to `requests.get()` to get a raw byte stream,\n",
    "        # because it's gzip compressed data\n",
    "        stream = ArchiveIterator(response.raw)\n",
    "        for warc_record in stream:\n",
    "            if warc_record.rec_type == 'response':\n",
    "                return warc_record.content_stream().read()\n",
    "    else:\n",
    "        print(f\"Failed to fetch data: {response.status_code}\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def append_df_row_to_pickle(row, pickle_file):\n",
    "    \"\"\"\n",
    "    Append a row to a DataFrame stored in a pickle file.\n",
    "    \n",
    "    Arguments:\n",
    "        row {pd.Series} -- The row to be appended to the DataFrame.\n",
    "        pickle_file {str} -- The path to the pickle file where the DataFrame is stored.\n",
    "    \"\"\"\n",
    "    # Check if the pickle file exists\n",
    "    if os.path.exists(pickle_file):\n",
    "        # Load the existing DataFrame from the pickle file\n",
    "        df = pd.read_pickle(pickle_file)\n",
    "    else:\n",
    "        # If the file doesn't exist, create a new DataFrame\n",
    "        df = pd.DataFrame(columns=row.index)\n",
    "\n",
    "    # Append the new row to the DataFrame\n",
    "    df = pd.concat([df, pd.DataFrame([row])], ignore_index=True)\n",
    "    \n",
    "    # Save the updated DataFrame back to the pickle file\n",
    "    df.to_pickle(pickle_file)\n",
    "\n",
    "\n",
    "def load_processed_indices(pickle_file):\n",
    "    \"\"\"\n",
    "    Load processed indices from a pickle file to check previously processed records.\n",
    "\n",
    "    Arguments:\n",
    "        pickle_file {str} -- The path to the pickle file where the DataFrame is stored.\n",
    "    \n",
    "    Returns:\n",
    "        Set of processed indices.\n",
    "    \"\"\"\n",
    "    if os.path.exists(pickle_file):\n",
    "        df = pd.read_pickle(pickle_file)\n",
    "        # Assuming 'index' column is in the DataFrame and contains indices of processed records\n",
    "        processed_indices = set(df['index'].unique())\n",
    "        print(f\"Loaded {len(processed_indices)} processed indices from {pickle_file}\")\n",
    "        return processed_indices\n",
    "    else:\n",
    "        print(f\"No processed indices found. Pickle file '{pickle_file}' does not exist.\")\n",
    "        return set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9f331c4-9400-4d3a-8a3a-f3792ab51237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running:  CC-MAIN-2024-33\n",
      "Running:  CC-MAIN-2024-30\n",
      "Running:  CC-MAIN-2024-26\n"
     ]
    }
   ],
   "source": [
    "# The URL you want to look up in the Common Crawl index\n",
    "target_url = 'https://www.slotzuylen.nl/*'  # Replace with your target URL\n",
    "\n",
    "# list of indexes https://commoncrawl.org/get-started\n",
    "indexes  = ['CC-MAIN-2024-33','CC-MAIN-2024-30','CC-MAIN-2024-26']\n",
    "\n",
    "record_dfs = []\n",
    "\n",
    "# Fetch each index and store into a dataframe\n",
    "for index_name in indexes:\n",
    "    print('Running: ', index_name)\n",
    "    records = search_cc_index(target_url,index_name)\n",
    "    record_df = pd.DataFrame(records)\n",
    "    record_df['index_name'] = index_name\n",
    "    record_dfs.append(record_df)\n",
    "\n",
    "# Combine individual dataframes\n",
    "all_records_df = pd.concat(record_dfs)\n",
    "all_records_df = all_records_df.sort_values(by='index_name', ascending=False)\n",
    "all_records_df = all_records_df.reset_index()\n",
    "\n",
    "# Create columns where to store data later\n",
    "all_records_df['success_status'] = 'not processed'\n",
    "all_records_df['html'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cddeac1a-b905-4882-89bc-6f2f6dbecc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records_df.to_csv('all_records_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18947985-f854-40b6-bb6d-e7eed07e6436",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = all_records_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a980ce4-edeb-4cf5-ab99-d6f0e3b857b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100 processed indices from commcrawl_indeed.pkl\n",
      "Found 235 records for https://www.slotzuylen.nl/*\n"
     ]
    }
   ],
   "source": [
    "# safeguareds\n",
    "# If pickle file exists, check for processed items\n",
    "pickle_file = 'commcrawl_indeed.pkl'\n",
    "processed_indices = load_processed_indices(pickle_file)\n",
    "#if processed_indices:\n",
    "#    # Remove processed items\n",
    "#    df = df[~df['index'].isin(processed_indices)]\n",
    "\n",
    "# Create storage for later\n",
    "successful = set()\n",
    "results = {}\n",
    "\n",
    "# Keep track of each row processed\n",
    "i = 0 \n",
    "perc = 0\n",
    "n_records = len(df)\n",
    "print(f\"Found {n_records} records for {target_url}\")\n",
    "mod = int(n_records * 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c5d2d0d-2767-481a-8ad8-7ef59e81f7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 235: 0%\n",
      "2 of 235: 1%\n",
      "4 of 235: 2%\n",
      "6 of 235: 3%\n",
      "8 of 235: 4%\n",
      "10 of 235: 5%\n",
      "12 of 235: 6%\n",
      "14 of 235: 7%\n",
      "16 of 235: 8%\n",
      "18 of 235: 9%\n",
      "20 of 235: 10%\n",
      "22 of 235: 11%\n",
      "24 of 235: 12%\n",
      "26 of 235: 13%\n",
      "28 of 235: 14%\n",
      "30 of 235: 15%\n",
      "32 of 235: 16%\n",
      "34 of 235: 17%\n",
      "36 of 235: 18%\n",
      "38 of 235: 19%\n",
      "40 of 235: 20%\n",
      "42 of 235: 21%\n",
      "44 of 235: 22%\n",
      "46 of 235: 23%\n",
      "48 of 235: 24%\n",
      "50 of 235: 25%\n",
      "52 of 235: 26%\n",
      "54 of 235: 27%\n",
      "56 of 235: 28%\n",
      "58 of 235: 29%\n",
      "60 of 235: 30%\n",
      "62 of 235: 31%\n",
      "64 of 235: 32%\n",
      "66 of 235: 33%\n",
      "68 of 235: 34%\n",
      "70 of 235: 35%\n",
      "72 of 235: 36%\n",
      "74 of 235: 37%\n",
      "76 of 235: 38%\n",
      "78 of 235: 39%\n",
      "80 of 235: 40%\n",
      "82 of 235: 41%\n",
      "Failed to fetch data: 503\n",
      "84 of 235: 42%\n",
      "86 of 235: 43%\n",
      "Failed to fetch data: 503\n",
      "88 of 235: 44%\n",
      "90 of 235: 45%\n",
      "Failed to fetch data: 503\n",
      "92 of 235: 46%\n",
      "94 of 235: 47%\n",
      "96 of 235: 48%\n",
      "98 of 235: 49%\n",
      "100 of 235: 50%\n",
      "102 of 235: 51%\n",
      "104 of 235: 52%\n",
      "106 of 235: 53%\n",
      "108 of 235: 54%\n",
      "110 of 235: 55%\n",
      "112 of 235: 56%\n",
      "114 of 235: 57%\n",
      "116 of 235: 58%\n",
      "118 of 235: 59%\n",
      "120 of 235: 60%\n",
      "122 of 235: 61%\n",
      "124 of 235: 62%\n",
      "126 of 235: 63%\n",
      "128 of 235: 64%\n",
      "Failed to fetch data: 503\n",
      "130 of 235: 65%\n",
      "132 of 235: 66%\n",
      "134 of 235: 67%\n",
      "136 of 235: 68%\n",
      "138 of 235: 69%\n",
      "140 of 235: 70%\n",
      "142 of 235: 71%\n",
      "144 of 235: 72%\n",
      "146 of 235: 73%\n",
      "148 of 235: 74%\n",
      "150 of 235: 75%\n",
      "152 of 235: 76%\n",
      "154 of 235: 77%\n",
      "156 of 235: 78%\n",
      "158 of 235: 79%\n",
      "160 of 235: 80%\n",
      "162 of 235: 81%\n",
      "164 of 235: 82%\n",
      "166 of 235: 83%\n",
      "168 of 235: 84%\n",
      "170 of 235: 85%\n",
      "172 of 235: 86%\n",
      "174 of 235: 87%\n",
      "176 of 235: 88%\n",
      "178 of 235: 89%\n",
      "180 of 235: 90%\n",
      "182 of 235: 91%\n",
      "184 of 235: 92%\n",
      "186 of 235: 93%\n",
      "188 of 235: 94%\n",
      "190 of 235: 95%\n",
      "Failed to fetch data: 503\n",
      "192 of 235: 96%\n",
      "194 of 235: 97%\n",
      "196 of 235: 98%\n",
      "198 of 235: 99%\n",
      "200 of 235: 100%\n",
      "202 of 235: 101%\n",
      "204 of 235: 102%\n",
      "206 of 235: 103%\n",
      "208 of 235: 104%\n",
      "210 of 235: 105%\n",
      "212 of 235: 106%\n",
      "214 of 235: 107%\n",
      "216 of 235: 108%\n",
      "218 of 235: 109%\n",
      "220 of 235: 110%\n",
      "222 of 235: 111%\n",
      "224 of 235: 112%\n",
      "226 of 235: 113%\n",
      "228 of 235: 114%\n",
      "230 of 235: 115%\n",
      "232 of 235: 116%\n",
      "234 of 235: 117%\n"
     ]
    }
   ],
   "source": [
    "from warcio.archiveiterator import ArchiveIterator\n",
    "\n",
    "# Reset index to help with looping\n",
    "df.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "for i in range(len(df)):\n",
    "    # Print every 1% process\n",
    "    if i % mod == 0: \n",
    "        print(f'{i} of {n_records}: {perc}%')\n",
    "        perc += 1\n",
    "\n",
    "    record_url = df.loc[i, 'url']\n",
    "\n",
    "    # Fetch only URLs that were not processed\n",
    "    # If it was already processed, skip URL \n",
    "    # (Helps speeding if you only need one version of the HTML, not its history)\n",
    "    if not record_url in successful:\n",
    "        length = int(df.loc[i, 'length'])\n",
    "        offset = int(df.loc[i, 'offset'])\n",
    "        warc_record_filename = df.loc[i, 'filename']\n",
    "        result = fetch_single_record(warc_record_filename, offset, length)\n",
    "        \n",
    "        if not result:\n",
    "            df.loc[i,'success_status'] = 'invalid warc'\n",
    "        else:\n",
    "            df.loc[i,'success_status'] = 'success'\n",
    "            df.loc[i,'html'] = result\n",
    "    else: \n",
    "        df.loc[i,'success_status'] = 'previously processed'\n",
    "\n",
    "    # Add to pickle file\n",
    "    append_df_row_to_pickle(df.loc[i, :], pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "762c165f-9cf2-4942-8693-eeb130e5a508",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>filename</th>\n",
       "      <th>index_name</th>\n",
       "      <th>success_status</th>\n",
       "      <th>html</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://slotzuylen.nl/</td>\n",
       "      <td>crawl-data/CC-MAIN-2024-33/segments/1722640427...</td>\n",
       "      <td>CC-MAIN-2024-33</td>\n",
       "      <td>success</td>\n",
       "      <td>b'    &lt;!doctype html&gt;\\r\\n&lt;html lang=\"nl-NL\"&gt;\\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://slotzuylen.nl/agenda/overzicht/?relate...</td>\n",
       "      <td>crawl-data/CC-MAIN-2024-33/segments/1722640427...</td>\n",
       "      <td>CC-MAIN-2024-33</td>\n",
       "      <td>success</td>\n",
       "      <td>b'    &lt;!doctype html&gt;\\r\\n&lt;html lang=\"nl-NL\"&gt;\\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://slotzuylen.nl/bewoners/</td>\n",
       "      <td>crawl-data/CC-MAIN-2024-33/segments/1722640427...</td>\n",
       "      <td>CC-MAIN-2024-33</td>\n",
       "      <td>success</td>\n",
       "      <td>b'    &lt;!doctype html&gt;\\r\\n&lt;html lang=\"nl-NL\"&gt;\\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://slotzuylen.nl/agenda/zomeravondrondlei...</td>\n",
       "      <td>crawl-data/CC-MAIN-2024-33/segments/1722640427...</td>\n",
       "      <td>CC-MAIN-2024-33</td>\n",
       "      <td>success</td>\n",
       "      <td>b'    &lt;!doctype html&gt;\\r\\n&lt;html lang=\"nl-NL\"&gt;\\r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://slotzuylen.nl/agenda/zomeravondrondlei...</td>\n",
       "      <td>crawl-data/CC-MAIN-2024-33/segments/1722640427...</td>\n",
       "      <td>CC-MAIN-2024-33</td>\n",
       "      <td>success</td>\n",
       "      <td>b'    &lt;!doctype html&gt;\\r\\n&lt;html lang=\"nl-NL\"&gt;\\r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0                             https://slotzuylen.nl/   \n",
       "1  https://slotzuylen.nl/agenda/overzicht/?relate...   \n",
       "2                    https://slotzuylen.nl/bewoners/   \n",
       "3  https://slotzuylen.nl/agenda/zomeravondrondlei...   \n",
       "4  https://slotzuylen.nl/agenda/zomeravondrondlei...   \n",
       "\n",
       "                                            filename       index_name  \\\n",
       "0  crawl-data/CC-MAIN-2024-33/segments/1722640427...  CC-MAIN-2024-33   \n",
       "1  crawl-data/CC-MAIN-2024-33/segments/1722640427...  CC-MAIN-2024-33   \n",
       "2  crawl-data/CC-MAIN-2024-33/segments/1722640427...  CC-MAIN-2024-33   \n",
       "3  crawl-data/CC-MAIN-2024-33/segments/1722640427...  CC-MAIN-2024-33   \n",
       "4  crawl-data/CC-MAIN-2024-33/segments/1722640427...  CC-MAIN-2024-33   \n",
       "\n",
       "  success_status                                               html  \n",
       "0        success  b'    <!doctype html>\\r\\n<html lang=\"nl-NL\">\\r...  \n",
       "1        success  b'    <!doctype html>\\r\\n<html lang=\"nl-NL\">\\r...  \n",
       "2        success  b'    <!doctype html>\\r\\n<html lang=\"nl-NL\">\\r...  \n",
       "3        success  b'    <!doctype html>\\r\\n<html lang=\"nl-NL\">\\r...  \n",
       "4        success  b'    <!doctype html>\\r\\n<html lang=\"nl-NL\">\\r...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commoncrawl_data = pd.read_pickle(pickle_file)\n",
    "commoncrawl_data[\n",
    "    ['url','filename','index_name','success_status','html']\n",
    "    ].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f52bb1d7-75f7-42d2-adec-11aa44bba3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Kasteel - Slot Zuylen</title>\n"
     ]
    }
   ],
   "source": [
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "# Select HTML from row 0\n",
    "content = commoncrawl_data.loc[0, 'html']\n",
    "\n",
    "# Parse in Beautiful soup\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "print(soup.find('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d906ca-273c-4b4f-b863-d25719b69a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
